{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workwheet Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,1],[1,1]])\n",
    "e = np.array([0,-1])\n",
    "w = np.array([1,-2])\n",
    "b = 0\n",
    "#x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "x = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = max(np.transpose(W).dot(x) + e) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "# N is the sample size - 150\n",
    "# Di is the number of input features - 4\n",
    "# Do is the number of output features - 3\n",
    "# H1 is the number of units in the hidden layer - 10\n",
    "[N, Di] = X.shape # N objects, Di input dimension\n",
    "Do = 3 # output dimension\n",
    "H1 = 10 # dimension of hidden layer\n",
    "alpha = 0.00001 # set the learning rate for gradient descent\n",
    "y = np.reshape(y, (N,1)) # convert y to a vector y.shape = (150,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Note that the number of hidden units H1 is set to 10. This can be changed if wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the set of categories in y to a set of one hot variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabio\\Anaconda3\\envs\\openpose\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y = onehot_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of preparation is to create the set of neural network parameters (weight matrices). We\n",
    "are creating a two layer network, therefore, weights are needed from the input layer to the hidden layer,\n",
    "and the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.random.randn(Di, H1)\n",
    "w1 = np.random.randn(H1, Do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple linear loss function, where we assume that the response Ë†y is Normal, and so need\n",
    "to maximise the likelihood of a Normal distribution with constant variance. This provides the mean\n",
    "squared error loss and loss gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearLoss(y, yhat):\n",
    "    return(np.square(yhat - y).sum())\n",
    "def linearLossGrad(y, yhat):\n",
    "    return(2*(yhat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function and  Linear function\n",
    "lossFunction = linearLoss\n",
    "lossGrad = linearLossGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return(np.maximum(x, 0))\n",
    "def reluGrad(x):\n",
    "    if (x < 0):\n",
    "        return(0)\n",
    "    return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = np.vectorize(relu)\n",
    "activationGrad = np.vectorize(reluGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation(np.array([[1,-2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activationGrad(np.array([[1,-2],[3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training consists of repeatedly performing:\n",
    "1. forward propagation to compute the loss, then\n",
    "2. back propagation to compute the gradient of the loss with respect to the weights, then\n",
    "3. update the weights using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation to compute the estimate of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = X.dot(w2) # multiply input by weights and sum #  # shape (150,10)\n",
    "fh1 = activation(h1) # make sure activation function is chosen\n",
    "yhat = fh1.dot(w1) # output will be shape(150x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.72234014,  0.        ,  0.        ,  0.        ,  9.50872106,\n",
       "        0.        ,  7.89953289,  0.        ,  1.73176463,  1.65897082])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fh1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.48931178,  -3.239756  , -17.06118292],\n",
       "       [ -6.92492996,  -2.79802819, -15.67824111],\n",
       "       [ -6.8458251 ,  -2.93272115, -15.63932592]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossFunction(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49173.2604865844"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ iter: 825 ] loss: 58.1873513529359558 ] loss: 444.14766259367093] loss: 423.30508189156467 21 ] loss: 385.88300655393004 27 ] loss: 300.22112296199396 29 ] loss: 278.41330413325943 30 ] loss: 268.5002901513386 32 ] loss: 250.4213871634896 35 ] loss: 227.07945430049662 37 ] loss: 213.64749426514823 39 ] loss: 201.66148614692335 40 ] loss: 196.15217142907505 42 ] loss: 185.99897835576428 45 ] loss: 172.6703616352517 47 ] loss: 164.876319336420848 ] loss: 161.26786558997134 54 ] loss: 142.99570555035 55 ] loss: 140.43109954332698 57 ] loss: 135.6470466714373 63 ] loss: 123.63851298382824 68 ] loss: 115.76097918783401 70 ] loss: 113.04001427451607 75 ] loss: 107.11605983473913] loss: 105.04982897596472 79 ] loss: 103.1334772376784 83 ] loss: 99.69820962452044 84 ] loss: 98.91377199942379 86 ] loss: 97.42519698473076 88 ] loss: 96.03611801777424 89 ] loss: 95.3762626947664 90 ] loss: 94.73823338643712 94 ] loss: 92.386782026226291.84505352079078 100 ] loss: 89.37793327294902 103 ] loss: 88.06914799958285 104 ] loss: 87.65824910818442 ] loss: 85.77345029985001 113 ] loss: 84.4455775736189114 ] loss: 84.13569366349162 116 ] loss: 83.54028376042137 ] loss: 82.70393489015595 121 ] loss: 82.18125815503758 125 ] loss: 81.21149155353821 127 ] loss: 80.76116208696979 128 ] loss: 80.54398811554447 131 ] loss: 79.92241649354305 134 ] loss: 79.34253899545969 140 ] loss: 78.29314368229518141 ] loss: 78.13114119723303 142 ] loss: 77.97253244488819 144 ] loss: 77.6650974417035 146 ] loss: 77.37007551359349 149 ] loss: 76.94927404115859 150 ] loss: 76.8144728762488 153 ] loss: 76.42540094695676 161 ] loss: 75.48846660897215 170 ] loss: 74.58029871874905 172 ] loss: 74.39613758352257 179 ] loss: 73.7947744254992 184 ] loss: 73.4017976545574 187 ] loss: 73.17890871086826 190 ] loss: 72.9648584373438572.82676159034119 198 ] loss: 72.4327293437664 72.36980991563824 201 ] loss: 72.24617016891942 204 ] loss: 72.06596226157043 206 ] loss: 71.94914343824291 211 ] loss: 71.66787835170348 214 ] loss: 71.50598578387259 215 ] loss: 71.45309249961056 217 ] loss: 71.34884389581967 219 ] loss: 71.2465737525934 220 ] loss: 71.19615456767045 222 ] loss: 71.09669818132797 225 ] loss: 70.95082658564183226 ] loss: 70.90304996302739 228 ] loss: 70.80871592607465 ] loss: 70.67014082729635 234 ] loss: 70.53489217051629 236 ] loss: 70.44647796796488 239 ] loss: 70.31634880561856242 ] loss: 70.18905849717883 244 ] loss: 70.10569611002494 245 ] loss: 70.06444736233999246 ] loss: 70.02347971166733248 ] loss: 69.9423668625131 252 ] loss: 69.78329185104519 255 ] loss: 69.6665948982792 263 ] loss: 69.36540390971317 266 ] loss: 69.25591345337742 271 ] loss: 69.07725344340965 277 ] loss: 68.8687062857006280 ] loss: 68.76665100342336 283 ] loss: 68.66598486138514 284 ] loss: 68.63272736449319 286 ] loss: 68.56664596809583 289 ] loss: 68.46857638948111 292 ] loss: 68.37172187213201297 ] loss: 68.21286120849331 299 ] loss: 68.15017365666765 304 ] loss: 67.99548434523551 67.9040032465993 309 ] loss: 67.84354788743177 312 ] loss: 67.75363694173805 ] loss: 67.72386756181328 315 ] loss: 67.66462307111996 317 ] loss: 67.60576333157462 320 ] loss: 67.51817595560814] loss: 67.48916321737032 324 ] loss: 67.40266008710745 325 ] loss: 67.37400085219338 330 ] loss: 67.2319768753742467.0089434770619 340 ] loss: 66.95395462885706 343 ] loss: 66.87202515893605] loss: 66.79074429464129 348 ] loss: 66.73690892862366 350 ] loss: 66.68334941147242351 ] loss: 66.65667163704589 353 ] loss: 66.6035172300817 354 ] loss: 66.57703949740986 358 ] loss: 66.4717793236116 360 ] loss: 66.4195325544708 363 ] loss: 66.3416305748618 66.3157862280401766.23861584343932 369 ] loss: 66.18746681594371 376 ] loss: 66.01027045531015 379 ] loss: 65.93517527684747 389 ] loss: 65.68837228782539 392 ] loss: 65.61535233197108 396 ] loss: 65.51870183293767 399 ] loss: 65.44673641241438 404 ] loss: 65.32776835890209 410 ] loss: 65.18657668474937 413 ] loss: 65.11660860294695 415 ] loss: 65.07019171664412 417 ] loss: 65.02395568081371 418 ] loss: 65.00090497265893 421 ] loss: 64.9320197046326 423 ] loss: 64.88631663665474 424 ] loss: 64.86353066743864 426 ] loss: 64.81808893405909 429 ] loss: 64.75024913896542430 ] loss: 64.72772124804877 432 ] loss: 64.68279249111951 434 ] loss: 64.6380319755644 446 ] loss: 64.37292981473927 448 ] loss: 64.32931149150545 64.17788758714721 458 ] loss: 64.11357405018481 460 ] loss: 64.07088986822689 462 ] loss: 64.02835770518011 64.0071483180945 465 ] loss: 63.96484231714142 467 ] loss: 63.922685872071405 470 ] loss: 63.85972953580746475 ] loss: 63.755536776647475 63.693456821783414 485 ] loss: 63.54985527110835 487 ] loss: 63.50914430974153 489 ] loss: 63.46857319403746 493 ] loss: 63.38784728974297 496 ] loss: 63.327664047668954 499 ] loss: 63.267787500608705 501 ] loss: 63.22803892812970663.208215100032824 505 ] loss: 63.14894434603428 507 ] loss: 63.10959687806158 509 ] loss: 63.070381668063234 512 ] loss: 63.01180529179811 514 ] loss: 62.97291765034853 525 ] loss: 62.761338085918865 526 ] loss: 62.74229447116812 528 ] loss: 62.70430155062512 529 ] loss: 62.68535208562833530 ] loss: 62.666433845353595 532 ] loss: 62.62869072497586537 ] loss: 62.53487390245164 539 ] loss: 62.49756195445747 541 ] loss: 62.460371830369944 544 ] loss: 62.404813758372825 546 ] loss: 62.36792559168633 548 ] loss: 62.3311571812622 62.276227843913496 553 ] loss: 62.23975630307696 557 ] loss: 62.16716591780557 560 ] loss: 62.1130295393794 562 ] loss: 62.07708349516993 563 ] loss: 62.05915373061487 565 ] loss: 62.023380377927836 567 ] loss: 61.98772147874858 568 ] loss: 61.969934782084295 570 ] loss: 61.934446562638826 ] loss: 61.91674490785151 573 ] loss: 61.88142618005624 581 ] loss: 61.74126952113365 582 ] loss: 61.72387468493429] loss: 61.68916748034995 585 ] loss: 61.67185498676049 589 ] loss: 61.6028776202666 594 ] loss: 61.51726466850599 598 ] loss: 61.44925657539862 601 ] loss: 61.398529313539726 603 ] loss: 61.36484301713881 606 ] loss: 61.3145102184156261.297784825231844 609 ] loss: 61.264412002003255 615 ] loss: 61.164913121001945 617 ] loss: 61.13195184593337 621 ] loss: 61.06633413459925 ] loss: 61.01738584141089 626 ] loss: 60.98487900871509 627 ] loss: 60.96866303786702 629 ] loss: 60.936305716651844 631 ] loss: 60.9040475306754 633 ] loss: 60.87188805177722 636 ] loss: 60.82383297905772 638 ] loss: 60.791918411818514 639 ] loss: 60.77599761517187 640 ] loss: 60.76010107358867 642 ] loss: 60.72838054779905 644 ] loss: 60.69675642037895 649 ] loss: 60.61811514959249 654 ] loss: 60.540067437948316 658 ] loss: 60.47805241353705 665 ] loss: 60.37042102751434 668 ] loss: 60.32463854146144 670 ] loss: 60.29423100336206 672 ] loss: 60.26391431512453 674 ] loss: 60.23368809727276 675 ] loss: 60.2186087967399 677 ] loss: 60.188517577616864 60.14354850320068460.0987796199928 685 ] loss: 60.06904430064821 687 ] loss: 60.03939703633536 689 ] loss: 60.009837463153204690 ] loss: 59.99509044756514 692 ] loss: 59.965661733140585 697 ] loss: 59.89246887345171 699 ] loss: 59.863342366919866704 ] loss: 59.7908996127749159.776474743566276707 ] loss: 59.74768834008198 709 ] loss: 59.71898609621076 711 ] loss: 59.69036766921204 714 ] loss: 59.6475964401159 716 ] loss: 59.61918606781288 718 ] loss: 59.59085832714623 720 ] loss: 59.56261288352245 723 ] loss: 59.52039829748021 726 ] loss: 59.47836701549798 728 ] loss: 59.450447449386246 735 ] loss: 59.353361967024796 736 ] loss: 59.33957245616461 738 ] loss: 59.312052917620626 742 ] loss: 59.25725066441738 745 ] loss: 59.216354956079634 749 ] loss: 59.16209998436332 750 ] loss: 59.1485846689673 59.081296737810476 765 ] loss: 58.94814894567279 768 ] loss: 58.90857152593226 774 ] loss: 58.82991842197846 779 ] loss: 58.76488053576407 784 ] loss: 58.70029819973314 789 ] loss: 58.63616698837064 790 ] loss: 58.62339449551581 792 ] loss: 58.59790298110909 58.58518389041956 795 ] loss: 58.559798869871216 796 ] loss: 58.547132871391405804 ] loss: 58.44643667135753 808 ] loss: 58.39650625437074 811 ] loss: 58.35923942417444 815 ] loss: 58.30978993257129 816 ] loss: 58.29747012965259 818 ] loss: 58.27288137981222 820 ] loss: 58.24836022204302 823 ] loss: 58.21170465650003 826 ] loss: 58.175199727507405\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ iter: 998 ] loss: 56.308398216691426830 ] loss: 58.1267593284703 833 ] loss: 58.090602554797655 835 ] loss: 58.06658020052326 837 ] loss: 58.04262328513116 838 ] loss: 58.03066928962884 844 ] loss: 57.95928585015969 847 ] loss: 57.92381167119343 849 ] loss: 57.90024223362256 850 ] loss: 57.8884814291042 851 ] loss: 57.876736527486614 854 ] loss: 57.84159694172161 856 ] loss: 57.81824952005735 857 ] loss: 57.80659941113572 859 ] loss: 57.78334624991851 860 ] loss: 57.771743138957035 862 ] loss: 57.74858371019103] loss: 57.737027334141715 865 ] loss: 57.7139611136214 867 ] loss: 57.690956742562925 868 ] loss: 57.67947767859994 870 ] loss: 57.65656565044384 872 ] loss: 57.63371489829544 ] loss: 57.622312429597166 877 ] loss: 57.576854610344796 878 ] loss: 57.56552802859681 882 ] loss: 57.520372353954556 885 ] loss: 57.486663004591264 888 ] loss: 57.45308774666273 891 ] loss: 57.41964584021686 893 ] loss: 57.39742495703734 897 ] loss: 57.353159148072535 899 ] loss: 57.33111379251042 901 ] loss: 57.30912651805895] loss: 57.25441104552842 908 ] loss: 57.232625389030765 909 ] loss: 57.22175399876028 911 ] loss: 57.20005396406073 913 ] loss: 57.17841075103823 918 ] loss: 57.12454996060406 921 ] loss: 57.09240200034192 923 ] loss: 57.07103981752854926 ] loss: 57.03910072501219 928 ] loss: 57.017877158642406 930 ] loss: 56.996708689074104 932 ] loss: 56.97559511782042 935 ] loss: 56.944027263343195 937 ] loss: 56.92305007549609 944 ] loss: 56.85005555444004 946 ] loss: 56.829320790075386 948 ] loss: 56.80863936868636949 ] loss: 56.798318602066296 951 ] loss: 56.777716838369116 952 ] loss: 56.767435794012464 954 ] loss: 56.74691326248899 956 ] loss: 56.72644331745431 959 ] loss: 56.69583659080867 963 ] loss: 56.65520993675117 969 ] loss: 56.594657638384106 979 ] loss: 56.49475884654744 980 ] loss: 56.48483851001394 982 ] loss: 56.4650354806127 984 ] loss: 56.445282495969785 985 ] loss: 56.43542471556998 987 ] loss: 56.4157464693199 989 ] loss: 56.3961178306501 990 ] loss: 56.38632205990889 992 ] loss: 56.36676750754186 999 ] loss: 56.29871270566481\r"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 1000):\n",
    "    \n",
    "    h1 = X.dot(w2) # multiply input by weights and sum #  # shape (150,10)\n",
    "    fh1 = activation(h1) # make sure activation function is chosen\n",
    "    yhat = fh1.dot(w1) # output will be shape(150x3)\n",
    "    loss = lossFunction(y, yhat)\n",
    "    \n",
    "    print('[ iter:', iteration, '] loss:', loss, end='\\r', flush=True)\n",
    "    \n",
    "    dJdy = lossGrad(y, yhat) # loss gradient\n",
    "    dydw1 = fh1\n",
    "    dJdw1 = dydw1.T.dot(dJdy)\n",
    "    \n",
    "    ## backpropagate the error compute the gradient with respect to the\n",
    "    ## second set of weights\n",
    "    dydh = w1\n",
    "    dhdg = activationGrad(h1)\n",
    "    dgdw2 = X\n",
    "    dJdh = dJdy.dot(dydh.T)\n",
    "    dJdg = dJdh*dhdg\n",
    "    dJdw2 = dgdw2.T.dot(dJdg)\n",
    "    \n",
    "    #update the weights using gradient descent\n",
    "    w1 = w1 - alpha*dJdw1\n",
    "    w2 = w2 - alpha*dJdw2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
